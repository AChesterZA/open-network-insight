#spot.conf should always be located in /etc/spot.conf
#file formated as standard .INI

[DEFAULT]
#base user and data source config
#HDFS
HUSER=/user/spot
DSOURCES=flow dns proxy
DFOLDERS=binary csv hive stage
DNS_PATH=%(HUSER)s/%(DSOURCE)s/hive/y=%(YR)s/m=%(MH)s/d=%(DY)s/
PROXY_PATH=%(HUSER)s/%(DSOURCE)s/hive/y=%(YR)s/m=%(MH)s/d=%(DY)s/
FLOW_PATH=%(HUSER)s/%(DSOURCE)s/hive/y=%(YR)s/m=%(MH)s/d=%(DY)s/
##removing FDATE for now*********
HPATH=%(HUSER)s/%(DSOURCE)s/scored_results/
#local fs
LUSER=/home/spot
LPATH=%(LUSER)s/ml/%(DSOURCE)s/%(FDATE)s
RPATH=%(LUSER)s/ipython/user/%(FDATE)s
LDAPATH=%(LUSER)s/ml/oni-lda-c
LIPATH=%(LUSER)s/ingest
#Spot architecture
#list where each spot component will run from
UINODE=node03
MLNODE=node04
GWNODE=node16

NODES=
    node-01
    node-02

[database]
IMPALA_DEM=node04
DBNAME=spot

[kerberos]
KRB_AUTH=false
KINITPATH=
KINITOPTS=
KEYTABPATH=
KRB_USER=

[spark]
SPK_CONFIG=false
SPK_EXEC=400
SPK_EXEC_MEM=2048m
SPK_DRIVER_MEM=None
SPK_DRIVER_MAX_RESULTS=None
SPK_EXEC_CORES=None
SPK_DRIVER_MEM_OVERHEAD=None
SPAK_EXEC_MEM_OVERHEAD=None
TOL=1e-6


[mpi]
#command to run MPI
MPI_CMD=mpiexec
#command to prepare system for MPI, eg. load environment variables
MPI_PREP_CMD=
#number of processes to run in MPI
PROCESS_COUNT=20